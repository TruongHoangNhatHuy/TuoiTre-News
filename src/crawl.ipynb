{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.remote.webelement import WebElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileHandler():\n",
    "    @staticmethod\n",
    "    def is_file_empty(file_name: str) -> bool:\n",
    "        \"\"\"Return True if file is empty\n",
    "\n",
    "        Args: \n",
    "            - file_name: file's name that is needed to be check\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return os.stat(file_name).st_size == 0\n",
    "        except(FileNotFoundError):\n",
    "            return True\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_csv(rows: list, file_name: str) -> None:\n",
    "        header = ['Title', 'Content', 'Date', 'Url', 'Summary']\n",
    "        data = pd.DataFrame(rows, index=None, columns=header)\n",
    "\n",
    "        if FileHandler.is_file_empty(file_name):\n",
    "            # remove unname cols\n",
    "            data.drop(data.filter(regex=\"Unname\"), axis=1, inplace=True)\n",
    "            # drop duplicate rows\n",
    "            data.drop_duplicates(inplace=True)\n",
    "            data.to_csv(file_name, index=False)\n",
    "        else:\n",
    "            existed_data = pd.read_csv(file_name)\n",
    "            # outer join\n",
    "            merged_data = pd.merge(data, existed_data, how='outer').sort_values(by='Date', axis=0, ascending=False)\n",
    "            # remove unname cols\n",
    "            merged_data.drop(merged_data.filter(regex=\"Unname\"), axis=1, inplace=True)\n",
    "            # drop duplicate rows\n",
    "            merged_data.drop_duplicates(inplace=True)\n",
    "            merged_data.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrowserOption(Enum):\n",
    "    \"\"\"Option for webbrowser\n",
    "    \"\"\"\n",
    "    EDGE = 1\n",
    "    CHROME = 2\n",
    "    FIREFOX = 3\n",
    "    SAFARI = 4\n",
    "\n",
    "class TuoiTre_Crawler:\n",
    "    @staticmethod\n",
    "    def get_driver(browser_option: BrowserOption = BrowserOption.EDGE):\n",
    "        \"\"\"Return driver depended on BrowserOption Enuml\n",
    "        \n",
    "        Args:\n",
    "            - browser_option: the option of browser's driver\n",
    "        \"\"\"\n",
    "        if browser_option == BrowserOption.EDGE:\n",
    "            options = webdriver.EdgeOptions()\n",
    "            options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "            options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "            return webdriver.ChromiumEdge(options=options)\n",
    "        elif browser_option == BrowserOption.FIREFOX:\n",
    "            return webdriver.Firefox()\n",
    "        elif browser_option == BrowserOption.SAFARI:\n",
    "            return webdriver.Safari()       \n",
    "        else:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "            options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "            return webdriver.Chrome(options=options)  \n",
    "        \n",
    "    def __init__(self, browser_option: BrowserOption, folder_to_save: str, category_url: dict) -> None:\n",
    "        \"\"\"Create a new instance of crawler\n",
    "        \n",
    "        Args:\n",
    "            - browser_option: the option of browser's driver\n",
    "            - folder_to_save: folder to save data crawled\n",
    "            - category_url: a dictionary which key=filename, value=category url\n",
    "        \"\"\"\n",
    "        self.driver = TuoiTre_Crawler.get_driver(browser_option)\n",
    "        # self.url_file = url_file\n",
    "        self.folder_to_save = folder_to_save\n",
    "        self.category_url = category_url\n",
    "\n",
    "    def crawl(self, url: str):\n",
    "        \"\"\"Crawl data from single url\n",
    "        \n",
    "        Args:\n",
    "            - url: a news url\n",
    "        \"\"\"\n",
    "        self.driver.get(url)\n",
    "\n",
    "        title_selector = \"#main-detail > .article-title\"\n",
    "        contents_selector = \"div.detail-content.afcbc-body > :not(.VCSortableInPreviewMode, #InreadPc)\"\n",
    "        date_selector = \"#main-detail > div.detail-top > div.detail-time\"\n",
    "        summary_selector = \"#main-detail > .detail-sapo\"\n",
    "        \n",
    "        title = self.driver.find_element(By.CSS_SELECTOR, title_selector)\n",
    "        contents = self.driver.find_elements(By.CSS_SELECTOR, contents_selector)\n",
    "        date = self.driver.find_element(By.CSS_SELECTOR, date_selector)\n",
    "        summary = self.driver.find_element(By.CSS_SELECTOR, summary_selector)\n",
    "\n",
    "        title = title.text.split('\\n')[0]\n",
    "        joined_content = \" \".join(x.text for x in contents)\n",
    "        return (title, joined_content, date.text, summary.text)\n",
    "\n",
    "    def news_from_category(self, filename: str, url: str):\n",
    "        \"\"\"Get news URL from category page\n",
    "        \n",
    "        Args:\n",
    "            - filename: file to save data\n",
    "            - url: a category page url\n",
    "        \"\"\"\n",
    "        self.driver.get(url)\n",
    "\n",
    "        focus_main_selector = \"div.list__focus-main a.box-category-link-title\"\n",
    "        focus_main = self.driver.find_elements(By.CSS_SELECTOR, focus_main_selector)\n",
    "\n",
    "        listing_main_selector = \"div.list__listing-main a.box-category-link-title\"\n",
    "        listing_main = self.driver.find_elements(By.CSS_SELECTOR, listing_main_selector)\n",
    "\n",
    "        news_urls = [url.get_property('href') for url in focus_main]\n",
    "        news_urls.extend([url.get_property('href') for url in listing_main])\n",
    "        \n",
    "        rows = []\n",
    "        for news_url in news_urls:\n",
    "            try:\n",
    "                (title, joined_content, date, summary) = self.crawl(news_url)\n",
    "                rows.append([title, joined_content, date, news_url, summary])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        FileHandler.write_to_csv(rows, os.path.join(self.folder_to_save, filename))\n",
    "        print(\"Crawled\", len(rows), \"from\", url)\n",
    "        return len(rows)\n",
    "\n",
    "    def start_crawl(self):\n",
    "        count = 0\n",
    "        for file, url in self.category_url.items():\n",
    "            count += self.news_from_category(file, url)\n",
    "\n",
    "        print(\"Done! Crawled\", count)\n",
    "        self.driver.quit()\n",
    "\n",
    "    def data_summary(self, verbose=False):\n",
    "        header = ['Title', 'Content', 'Date', 'Url', 'Summary']\n",
    "        total = pd.DataFrame(columns=header)\n",
    "        for file, url in self.category_url.items():\n",
    "            data = pd.read_csv(os.path.join(self.folder_to_save, file))\n",
    "            total = pd.merge(data, total, how='outer')\n",
    "            print('Data:', file)\n",
    "            print(data.info(verbose=verbose))\n",
    "            print('=====================================')\n",
    "        print('Total:')\n",
    "        print(total.info(verbose=verbose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawled 27 from https://tuoitre.vn/kinh-doanh.htm\n",
      "Crawled 29 from https://tuoitre.vn/cong-nghe.htm\n",
      "Crawled 17 from https://tuoitre.vn/du-lich.htm\n",
      "Crawled 22 from https://tuoitre.vn/van-hoa.htm\n",
      "Crawled 29 from https://tuoitre.vn/giai-tri.htm\n",
      "Crawled 28 from https://tuoitre.vn/the-thao.htm\n",
      "Crawled 28 from https://tuoitre.vn/giao-duc.htm\n",
      "Done! Crawled 180\n"
     ]
    }
   ],
   "source": [
    "category_url = {\n",
    "    \"tuoitre_kinhdoanh.csv\": \"https://tuoitre.vn/kinh-doanh.htm\",\n",
    "    \"tuoitre_congnghe.csv\": \"https://tuoitre.vn/cong-nghe.htm\",\n",
    "    \"tuoitre_dulich.csv\": \"https://tuoitre.vn/du-lich.htm\",\n",
    "    \"tuoitre_vanhoa.csv\": \"https://tuoitre.vn/van-hoa.htm\",\n",
    "    \"tuoitre_giaitri.csv\": \"https://tuoitre.vn/giai-tri.htm\",\n",
    "    \"tuoitre_thethao.csv\": \"https://tuoitre.vn/the-thao.htm\",\n",
    "    \"tuoitre_giaoduc.csv\": \"https://tuoitre.vn/giao-duc.htm\"\n",
    "}\n",
    "\n",
    "crawler = TuoiTre_Crawler(BrowserOption.EDGE, folder_to_save='./dataset/05-27', category_url=category_url)\n",
    "crawler.start_crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: tuoitre_kinhdoanh.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82 entries, 0 to 81\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    82 non-null     object\n",
      " 1   Content  82 non-null     object\n",
      " 2   Date     82 non-null     object\n",
      " 3   Url      82 non-null     object\n",
      " 4   Summary  82 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.3+ KB\n",
      "None\n",
      "=====================================\n",
      "Data: tuoitre_congnghe.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    46 non-null     object\n",
      " 1   Content  46 non-null     object\n",
      " 2   Date     46 non-null     object\n",
      " 3   Url      46 non-null     object\n",
      " 4   Summary  46 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.9+ KB\n",
      "None\n",
      "=====================================\n",
      "Data: tuoitre_dulich.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45 entries, 0 to 44\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    45 non-null     object\n",
      " 1   Content  45 non-null     object\n",
      " 2   Date     45 non-null     object\n",
      " 3   Url      45 non-null     object\n",
      " 4   Summary  45 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.9+ KB\n",
      "None\n",
      "=====================================\n",
      "Data: tuoitre_vanhoa.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    52 non-null     object\n",
      " 1   Content  52 non-null     object\n",
      " 2   Date     52 non-null     object\n",
      " 3   Url      52 non-null     object\n",
      " 4   Summary  52 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.2+ KB\n",
      "None\n",
      "=====================================\n",
      "Data: tuoitre_giaitri.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53 entries, 0 to 52\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    53 non-null     object\n",
      " 1   Content  53 non-null     object\n",
      " 2   Date     53 non-null     object\n",
      " 3   Url      53 non-null     object\n",
      " 4   Summary  53 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.2+ KB\n",
      "None\n",
      "=====================================\n",
      "Data: tuoitre_thethao.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 86 entries, 0 to 85\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    86 non-null     object\n",
      " 1   Content  86 non-null     object\n",
      " 2   Date     86 non-null     object\n",
      " 3   Url      86 non-null     object\n",
      " 4   Summary  86 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.5+ KB\n",
      "None\n",
      "=====================================\n",
      "Data: tuoitre_giaoduc.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80 entries, 0 to 79\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    80 non-null     object\n",
      " 1   Content  80 non-null     object\n",
      " 2   Date     80 non-null     object\n",
      " 3   Url      80 non-null     object\n",
      " 4   Summary  80 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.2+ KB\n",
      "None\n",
      "=====================================\n",
      "Total:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 433 entries, 0 to 432\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    433 non-null    object\n",
      " 1   Content  433 non-null    object\n",
      " 2   Date     433 non-null    object\n",
      " 3   Url      433 non-null    object\n",
      " 4   Summary  433 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 17.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "crawler.data_summary(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crawl(url: str) -> tuple[list[WebElement], list[WebElement]]:\n",
    "#     \"\"\"Crawl data from single url\n",
    "    \n",
    "#     Args:\n",
    "#         - url: a news url\n",
    "#     \"\"\"\n",
    "#     options = webdriver.EdgeOptions()\n",
    "#     options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "#     options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "#     driver = webdriver.ChromiumEdge(options=options)\n",
    "\n",
    "#     driver.get(url)\n",
    "\n",
    "#     title_selector = \"#main-detail > .article-title\"\n",
    "#     title = driver.find_element(By.CSS_SELECTOR, title_selector)\n",
    "#     print(title.text.split('\\n')[0])\n",
    "\n",
    "#     date_selector = \"#main-detail > div.detail-top > div.detail-time\"\n",
    "#     date = driver.find_element(By.CSS_SELECTOR, date_selector)\n",
    "#     print(date.text)\n",
    "\n",
    "#     summary_selector = \"#main-detail > .detail-sapo\"\n",
    "#     summary = driver.find_element(By.CSS_SELECTOR, summary_selector)\n",
    "#     print(summary.text)\n",
    "\n",
    "#     content_selector = \"div.detail-content.afcbc-body > :not(.VCSortableInPreviewMode, #InreadPc)\"\n",
    "#     content = driver.find_elements(By.CSS_SELECTOR, content_selector)\n",
    "#     joined_content = \" \".join(x.text+'\\n' for x in content)\n",
    "#     print(\"[\", joined_content, \"]\")\n",
    "\n",
    "# crawl('https://tuoitre.vn/thai-lan-dan-dau-sang-kien-visa-chung-o-dong-nam-a-20240413224000997.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def news_from_category(url: str):\n",
    "#     \"\"\"Get news URL from category page\n",
    "    \n",
    "#     Args:\n",
    "#         - url: a category page url\n",
    "#     \"\"\"\n",
    "#     options = webdriver.EdgeOptions()\n",
    "#     options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "#     options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "#     driver = webdriver.ChromiumEdge(options=options)\n",
    "\n",
    "#     driver.get(url)\n",
    "\n",
    "#     focus_main_selector = \"div.list__focus-main a.box-category-link-title\"\n",
    "#     focus_main = driver.find_elements(By.CSS_SELECTOR, focus_main_selector)\n",
    "#     print(\"focus_main\", len(focus_main))\n",
    "#     # for e in focus_main:\n",
    "#     #     print(e.get_property('href'))\n",
    "\n",
    "#     listing_main_selector = \"div.list__listing-main a.box-category-link-title\"\n",
    "#     listing_main = driver.find_elements(By.CSS_SELECTOR, listing_main_selector)\n",
    "#     print(\"listing_main\", len(listing_main))\n",
    "#     # for e in listing_main:\n",
    "#     #     print(e.get_property('href'))\n",
    "    \n",
    "#     news_url = [url.get_property('href') for url in focus_main]\n",
    "#     news_url.extend([url.get_property('href') for url in listing_main])\n",
    "#     print(len(news_url))\n",
    "#     for e in news_url:\n",
    "#         print(e)\n",
    "\n",
    "# news_from_category('https://tuoitre.vn/the-gioi.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def write_to_csv(rows: list, file_name):\n",
    "#     header = ['Title', 'Content', 'Date', 'Url', 'Summary']\n",
    "#     data = pd.DataFrame(rows, index=None, columns=header)\n",
    "\n",
    "#     if FileHandler.is_file_empty(file_name):\n",
    "#         data.to_csv(file_name)\n",
    "#     else:\n",
    "#         existed_data = pd.read_csv(file_name)\n",
    "#         merged_data = pd.merge(data, existed_data, how='outer').sort_values(by='Date', axis=0, ascending=False)\n",
    "#         merged_data.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
